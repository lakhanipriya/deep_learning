{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "environmental-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROBLEM 1-DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "advisory-judges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:  0.5 , threshold:  5\n",
      "epoch  1\n",
      "0   and   0  -> actual:  0 , predicted:  0  (weight:  0.0 )\n",
      "0   and   1  -> actual:  0 , predicted:  0  (weight:  0.0 )\n",
      "1   and   0  -> actual:  0 , predicted:  0  (weight:  0.0 )\n",
      "1   and   1  -> actual:  1 , predicted:  0  (weight:  0.5 )\n",
      "----------------------------------------------------------------\n",
      "epoch  2\n",
      "0   and   0  -> actual:  0 , predicted:  0  (weight:  0.5 )\n",
      "0   and   1  -> actual:  0 , predicted:  0  (weight:  0.5 )\n",
      "1   and   0  -> actual:  0 , predicted:  0  (weight:  0.5 )\n",
      "1   and   1  -> actual:  1 , predicted:  0  (weight:  1.0 )\n",
      "----------------------------------------------------------------\n",
      "epoch  3\n",
      "0   and   0  -> actual:  0 , predicted:  0  (weight:  1.0 )\n",
      "0   and   1  -> actual:  0 , predicted:  0  (weight:  1.0 )\n",
      "1   and   0  -> actual:  0 , predicted:  0  (weight:  1.0 )\n",
      "1   and   1  -> actual:  1 , predicted:  0  (weight:  1.5 )\n",
      "----------------------------------------------------------------\n",
      "epoch  4\n",
      "0   and   0  -> actual:  0 , predicted:  0  (weight:  1.5 )\n",
      "0   and   1  -> actual:  0 , predicted:  0  (weight:  1.5 )\n",
      "1   and   0  -> actual:  0 , predicted:  0  (weight:  1.5 )\n",
      "1   and   1  -> actual:  1 , predicted:  0  (weight:  2.0 )\n",
      "----------------------------------------------------------------\n",
      "epoch  5\n",
      "0   and   0  -> actual:  0 , predicted:  0  (weight:  2.0 )\n",
      "0   and   1  -> actual:  0 , predicted:  0  (weight:  2.0 )\n",
      "1   and   0  -> actual:  0 , predicted:  0  (weight:  2.0 )\n",
      "1   and   1  -> actual:  1 , predicted:  0  (weight:  2.5 )\n",
      "----------------------------------------------------------------\n",
      "epoch  6\n",
      "0   and   0  -> actual:  0 , predicted:  0  (weight:  2.5 )\n",
      "0   and   1  -> actual:  0 , predicted:  0  (weight:  2.5 )\n",
      "1   and   0  -> actual:  0 , predicted:  0  (weight:  2.5 )\n",
      "1   and   1  -> actual:  1 , predicted:  0  (weight:  3.0 )\n",
      "----------------------------------------------------------------\n",
      "epoch  7\n",
      "0   and   0  -> actual:  0 , predicted:  0  (weight:  3.0 )\n",
      "0   and   1  -> actual:  0 , predicted:  0  (weight:  3.0 )\n",
      "1   and   0  -> actual:  0 , predicted:  0  (weight:  3.0 )\n",
      "1   and   1  -> actual:  1 , predicted:  1  (weight:  3.0 )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "operator = 'and'\n",
    "\n",
    "#----------------\n",
    "\n",
    "atributes = np.array([ [0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "if operator == 'and':\n",
    "    labels = np.array([0, 0, 0, 1])\n",
    "elif operator == 'or':\n",
    "    labels = np.array([0, 1, 1, 1])\n",
    "elif operator == 'xor':\n",
    "    labels = np.array([0, 1, 1, 0])\n",
    "\n",
    "#----------------\n",
    "\n",
    "w = [+0, +0] #initial random values for weights\n",
    "\n",
    "threshold = 5\n",
    "alpha = 0.5 #learning rate\n",
    "epoch = 1000 #learning time\n",
    "#----------------\n",
    "\n",
    "print(\"learning rate: \", alpha,\", threshold: \", threshold)\n",
    "\n",
    "for i in range(0, epoch):\n",
    "    print(\"epoch \", i+1)\n",
    "    global_delta = 0 #this variable is used to terminate the for loop if learning completed in early epoch\n",
    "    for j in range(len(atributes)):\n",
    "        actual = labels[j]\n",
    "        sum = atributes[j][0]*w[0] + atributes[j][1]*w[1]\n",
    "        if sum > threshold: #then fire\n",
    "            predicted = 1\n",
    "        else: #do not fire\n",
    "            predicted = 0\n",
    "        delta = actual - predicted\n",
    "        global_delta = global_delta + abs(delta)\n",
    "        #update weights with respect to the error\n",
    "        for k in range(0, 2):\n",
    "            w[k] = w[k] + delta * alpha\n",
    "        print(atributes[j][0],\" \", operator, \" \", atributes[j][1], \" -> actual: \", actual, \", predicted: \", predicted, \" (weight: \",w[0],\")\")\n",
    "    if global_delta == 0:\n",
    "        break\n",
    "    print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "capital-control",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:  0.5 , threshold:  5\n",
      "epoch  1\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.1 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.1 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  0  (weight:  0.1 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  2\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  3\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  4\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  5\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  6\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  7\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  8\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  9\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  10\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  11\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  12\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  13\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  14\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  15\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  16\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  17\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  18\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  19\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  20\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  21\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  22\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  23\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  24\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  25\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  26\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  27\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  28\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  29\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  30\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  31\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  32\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  33\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  34\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  35\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  36\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  37\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  38\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  39\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  40\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  41\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  42\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  43\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  44\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  45\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  46\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  47\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  48\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  49\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n",
      "epoch  50\n",
      "\t 1   and   2  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 3   and   4  -> actual:  0 , predicted:  0  (weight:  0.6 )\n",
      "\t 5   and   6  -> actual:  0 , predicted:  1  (weight:  0.09999999999999998 )\n",
      "\t 7   and   8  -> actual:  1 , predicted:  0  (weight:  0.6 )\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE WITH DIFFERENT X AND Y VALUE\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#----------------\n",
    "atributes = np.array([ [1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "labels = np.array([0, 0, 0, 1])\n",
    "#----------------\n",
    "\n",
    "w = [0.1, 0.1] #initial random values for weights\n",
    "\n",
    "threshold = 5\n",
    "alpha = 0.5 #learning rate\n",
    "epoch = 50 #learning time\n",
    "#----------------\n",
    "\n",
    "print(\"learning rate: \", alpha,\", threshold: \", threshold)\n",
    "\n",
    "for i in range(0, epoch):\n",
    "    print(\"epoch \", i+1)\n",
    "    global_delta = 0 #this variable is used to terminate the for loop if learning completed in early epoch\n",
    "    for j in range(len(atributes)):\n",
    "        actual = labels[j]\n",
    "        sum = atributes[j][0]*w[0] + atributes[j][1]*w[1]\n",
    "        if sum > threshold: #then fire\n",
    "            predicted = 1\n",
    "        else: #do not fire\n",
    "            predicted = 0\n",
    "        delta = actual - predicted\n",
    "        global_delta = global_delta + abs(delta)\n",
    "        #update weights with respect to the error\n",
    "        for k in range(0, 2):\n",
    "            w[k] = w[k] + delta * alpha\n",
    "        print(\"\\t\",atributes[j][0],\" \", operator, \" \", atributes[j][1], \" -> actual: \", actual, \", predicted: \", predicted, \" (weight: \",w[0],\")\")\n",
    "    if global_delta == 0:\n",
    "        break\n",
    "    print(\"-----------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "beautiful-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "\tactivation = weights[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += weights[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "# test predictions\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "weights = [-0.1, 0.20653640140000007, -0.23418117710000003]\n",
    "for row in dataset:\n",
    "\tprediction = predict(row, weights)\n",
    "\tprint(\"Expected=%d, Predicted=%d\" % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "figured-diana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">epoch=0, lrate=0.100, error=2.000\n",
      "\n",
      ">epoch=1, lrate=0.100, error=1.000\n",
      "\n",
      ">epoch=2, lrate=0.100, error=0.000\n",
      "\n",
      ">epoch=3, lrate=0.100, error=0.000\n",
      "\n",
      ">epoch=4, lrate=0.100, error=0.000\n",
      "\n",
      "CALCULATED WEIGHTS : [-0.1, 0.20653640140000007, -0.23418117710000003]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "\tactivation = weights[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += weights[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0.0\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = predict(row, weights)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tsum_error += error**2\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "\t\tprint('\\n>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn weights\n",
    "\n",
    "# Calculate weights\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "l_rate = 0.1\n",
    "n_epoch = 5\n",
    "weights = train_weights(dataset, l_rate, n_epoch)\n",
    "print(\"\\nCALCULATED WEIGHTS :\",weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "sensitive-hamilton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">epoch=0, lrate=0.100, error=0.897\n",
      "\n",
      ">epoch=1, lrate=0.100, error=1.597\n",
      "\n",
      ">epoch=2, lrate=0.100, error=1.257\n",
      "\n",
      ">epoch=3, lrate=0.100, error=1.257\n",
      "\n",
      ">epoch=4, lrate=0.100, error=1.857\n",
      "\n",
      "CALCULATED WEIGHTS : [-0.06500000000000002, 0.08274999999999999, 0.10010000000000001]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "\tactivation = weights[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += weights[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0.0\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = predict(row, weights)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tsum_error += error**2\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "                #print('%f'%(weights[i + 1]))\n",
    "\t\tprint('\\n>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn weights\n",
    "\n",
    "# Calculate weights\n",
    "dataset = [[.1,.2,.3],\n",
    "\t[.4,.5,.6],\n",
    "\t[.7,.8,.9],\n",
    "\t[.10,.11,.12],\n",
    "\t[.13,.14,.15]]\n",
    "l_rate = 0.1\n",
    "n_epoch = 5\n",
    "weights = train_weights(dataset, l_rate, n_epoch)\n",
    "print(\"\\nCALCULATED WEIGHTS :\",weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "magnetic-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERCEPTRON CONVERGENCE FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "gothic-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "following-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAND gate features\n",
    "# note: x0 is a dummy variable for the bias term\n",
    "#     x0  x1  x2\n",
    "x = [[1., 0., 0.],                                  \n",
    "     [1., 0., 1.],                                 \n",
    "     [1., 1., 0.],                                  \n",
    "     [1., 1., 1.]] \n",
    "\n",
    "\n",
    "# Desired outputs\n",
    "y = [1.,                                            \n",
    "     1.,                                            \n",
    "     1.,                                            \n",
    "     0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "greatest-headquarters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights are:\n",
      "[ 0.2 -0.2 -0.1]\n",
      "\n",
      "The sum-of-squared erros are:\n",
      "[1.0, 1.5, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Training the Perceptron\n",
    "#\n",
    "# x:   feature data \n",
    "# y:   outputs \n",
    "# z:   threshold\n",
    "# eta: learning rate\n",
    "# t:   number of iterations\n",
    "\n",
    "def perceptron_train(x, y, z, eta, t):\n",
    "    \n",
    "    # Initializing parameters for the Perceptron\n",
    "    w = np.zeros(len(x[0]))        # weights \n",
    "    n = 0                          \n",
    "    \n",
    "    # Initializing additional parameters to compute SSE\n",
    "    yhat_vec = np.ones(len(y))     # vector for predictions\n",
    "    errors = np.ones(len(y))       # vector for errors (actual - predictions)\n",
    "    J = []                         # vector for the SSE cost function\n",
    "     \n",
    "        \n",
    "    while n < t:                                  \n",
    "        for i in range(0, len(x)):                 \n",
    "            \n",
    "            # summation step\n",
    "            f = np.dot(x[i], w)                      \n",
    "                        \n",
    "            # activation function\n",
    "            if f > z:                               \n",
    "                yhat = 1.                               \n",
    "            else:                                   \n",
    "                yhat = 0. \n",
    "            yhat_vec[i] = yhat                              \n",
    "        \n",
    "            # updating the weights\n",
    "            for j in range(0, len(w)):             \n",
    "                w[j] = w[j] + eta*(y[i]-yhat)*x[i][j]\n",
    "                \n",
    "            n += 1     \n",
    "\n",
    "        \n",
    "        # computing the sum-of-squared errors\n",
    "        for i in range(0,len(y)):     \n",
    "           errors[i] = (y[i]-yhat_vec[i])**2\n",
    "        J.append(0.5*np.sum(errors))\n",
    "           \n",
    "    # function returns the weight vector, and sum-of-squared errors        \n",
    "    return w, J\n",
    "\n",
    "\n",
    "z = 0.0     # threshold\n",
    "eta = 0.1   # learning rate\n",
    "t = 50      # number of iterations\n",
    "\n",
    "print(\"The weights are:\")\n",
    "print(perceptron_train(x, y, z, eta, t)[0])\n",
    "\n",
    "print(\"\\nThe sum-of-squared erros are:\")\n",
    "print(perceptron_train(x, y, z, eta, t)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "desperate-builder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Perceptron Convergence')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmyklEQVR4nO3deZxddX3/8dd7JpNkbtY7ZICQuTGgIAIC4gDuIi4gFdCqIO4opbaurb8W66+tWmt/Wqk/9OdCU4pY5ccOChYERAUroknYFxXKNkMCmWyQPZnJp3+cM8nNZObOmeXMmZn7fj4e98HZ7jmfcyfcz/2e76aIwMzM6ldD0QGYmVmxnAjMzOqcE4GZWZ1zIjAzq3NOBGZmdc6JwMyszjkRmJnVOScCy0zS45I2S9og6RlJ35U0s+i4ekn6vKQfjPE1Z0s6T9KT6efySLo+byzjMBsJJwIbqpMjYiZwFHA08LdDebMShfy7G+1rS5oK3AIcCpwIzAZeAawGjhmt64yUpClFx2DjmxOBDUtEPAXcABwGIOllkm6XtE7SPZKO6z1W0i8kfUnSr4BNwAGSDpV0s6Q1aenis+mxDZI+I+m/Ja2WdLmklnTfIkkh6WxJyyWtkPTpdN+JwGeB09Nf5vfUuPYrJC2R9Gz631f0ifWLkn4lab2km2r8un8/sBB4W0Q8GBE7ImJlRHwxIq5Pz/ei9JzrJD0g6ZSqa10k6VuS/jO91m8kPT/dd76kc6svJulHkv4yXd5P0lWSuiQ9JukTVcd9XtKVkn4g6Tngg5L2l3Rbep2fptf9QdV7Bvv7DfiZSHpV1Xs7JH0w3T5N0rlpaemZ9J6aB/gsrUgR4ZdfmV7A48Ab0uUK8ADwRWABya/gk0h+XLwxXW9Nj/0F8CTJL+cpwCxgBfBpYHq6fmx67KeAO4A2YBrwr8Al6b5FQACXADOAFwNdVTF9HvhBn5j7XnsfYC3wvnT9jHR9r6rj/xs4CGhO1788wOdxKfC9Gp9XE/AISYKaChwPrAdemO6/CFhDUnqYAlwMXJruew3QAShdLwObgf3Sz3gZ8PfpeQ8AHgVOqPoctgNvTY9tBn4NnJse/yrgud7PKuPfr9/PhCQRrk8/xyZgL+DIdN95wLVAS/o3vg74P0X/O/arn3+rRQfg18R5kSSCDcA64Ang2+kXwznA9/sceyPwgXT5F8A/VO07A7hrgGs8BLy+an1++qU2hV2J4OCq/f8M/Hu6/Hn6TwTV134f8Ns+x/wa+GDV8X9bte/PgZ8MEOvNDJAk0v2vBp4GGqq2XQJ8Pl2+CLigat9JwO/SZZEksNek638C/CxdPhZ4ss+1/gb4btXncFvVvoVAN1Cq2vYDdiWCLH+/fj+T9LrX9HPvAjYCz6/a9nLgsaL/Hfu158vPDm2o3hoRP63eIOl5wDslnVy1uQn4edV6R9VyheQXZn+eB1wjaUfVth6SX/L9nesJkpJBLdXH75e+p9oTJL+Kez1dtbwJGKhCfDVJohrIfkBHRFTfS6ZrRURIupQkad4GvJvkyxuSz2g/Seuq3tsI/LJqve89r4mITX32V6rON9jfb6DPZKC/ZStQApZJ6t2mNE4bZ1xHYKOhg+QX5dyq14yI+HLVMdHn+OfXONeb+5xreiR1Er0qVcsLgeX9XKNa9fblJF981RYCTzF0PwVOkDRjgP3LgUqfCuqhXOsS4B1poj0WuCrd3kHyy7r6M5oVESdVvbf6nlcALZJKVduqP8Msf7+BDPS3XEXyKOvQqnPOiaShgY0zTgQ2Gn4AnCzpBEmNkqZLOk5S2wDH/xjYV9Kn0grFWZKOTfedD3wp/fJDUqukU/u8/+8klSQdCpwJXJZufwZYNEjLoOuBgyS9W9IUSacDh6QxDdX3Sb4Ir5J0cFrRvZekz0o6CfgNyeORv5bUlFbAnkxStzCoiLiLpA7kAuDGiFiX7vot8JykcyQ1p5/5YZKOHuA8TwBLgc9Lmirp5WkcvYb696t2MfAGSaeln+deko5MS0H/BvxfSXsDSFog6YQs925jy4nARiwiOoBTSSpFu0i+HP+KAf59RcR6kgrJk0keOTwMvC7d/XWSCsabJK0nqTg+ts8pbiWphL0FODcibkq3X5H+d7WkOwe49mrgLSQV1auBvwbeEhGrhnDLvefaCrwB+B1JfcFzJF/S84DfRMQ24BTgzSS/kL8NvD8ifjeEy1ySXuP/V123h+SzOxJ4LD33BcCcGud5D8kz+tXAP5Ikz63p+Yb096sWEU+S1G18mqTi+27giHT3OSR/pzvS1ks/BV442Dlt7PW2SDAb9yQtIvnia4qI7oLDmdAkXUZSMf25omOx4rlEYFYHJB0t6fnp46sTSUoAPyw4LBsn3GrIrD7sC1xN0s6/E/iztA7CzI+GzMzqnR8NmZnVuQn3aGjevHmxaNGiosMwM5tQli1btioiWvvbN+ESwaJFi1i6dGnRYZiZTSiS+vao38mPhszM6pwTgZlZnXMiMDOrc04EZmZ1zonAzKzOORGYmdU5JwIzszo34foRjHc/vOspHu3akPt1Dlswhzcdum/u1zGzyc+JYBQ9uXoTn7rsbgB2zc43+iJgTnOTE4GZjQonglF05bIOJPjVOcez39zm3K6z+Lb/5p+u/x3Pbt7OnOam3K5jZvXBdQSjpGdHcMWyTl5zYGuuSQCgUk6mnu1Ys2mQI83MBudEMEr+65FVrHh2C6e1VwY/eITa0kTQudaJwMxGzolglFy+pINyqYk3HLJ37teqtCQljo41m3O/lplNfk4Eo2DNxm3c9ODTvPUlC5g2pTH3681pbmLWtCl0uERgZqPAiWAU/PCup9jeE5x+dP6PhQAk0dZSch2BmY0KJ4IRigguX9rBEW1zOHjf2WN23Uq5mc61fjRkZiPnRDBC9z31LL97ej3vHINK4mqVlhKdazfjOafNbKScCEbosiUdTJvSwClH7jem162Um9m8vYdVG7aN6XXNbPLJLRFIulDSSkn3D3Lc0ZJ6JL0jr1jysnlbD9fevZyTXjyf2dPHtmNXbxNSVxib2UjlWSK4CDix1gGSGoGvADfmGEdufvLACtZv7R6TvgN9VVrcqczMRkduiSAibgPWDHLYx4GrgJV5xZGny5d0srClxLH7t4z5tdvKSV8CVxib2UgVVkcgaQHwNuD8DMeeLWmppKVdXV35B5fBE6s38utHV3NaexsNDTmOMDeAGdOmsNeMqe5dbGYjVmRl8XnAORHRM9iBEbE4Itojor21tTX/yDK4clknDYK3v7StsBiSvgQuEZjZyBQ5+mg7cKmS8ZrnASdJ6o6IHxYYUyY9O4Irl3XymoNamT8n3wHmaqmUm7nvqWcLu76ZTQ6FlQgiYv+IWBQRi4ArgT+fCEkA4JcPd43ZAHO1tJVLLF+3mZ4d7ktgZsOXW4lA0iXAccA8SZ3A54AmgIgYtF5gPLt8aQctM6byhhftU2gclZZmtvcETz+3hQU5D31tZpNXbokgIs4YwrEfzCuO0bZm4zZufvAZ3v/yRUydUmx/vOp5CZwIzGy43LN4iK5JB5gr+rEQuC+BmY0OJ4IhiAiuWNrBEZW5vHDfWUWHw35zpyO5L4GZjYwTwRDc25kMMHdae3FNRqtNm9LIvrOne5gJMxsRJ4IhuGxpB9ObGjj5iLEdYK6WSrlEp/sSmNkIOBFktHlbD9fdvZyTDhv7AeZqaSs3u0RgZiPiRJDRDfenA8yN0SxkWbW1lHj6uS1s7R60g7aZWb+cCDK6fGkHz9urmAHmaqmUm4mA5eu2FB2KmU1QTgQZPLF6I3c8uobT2iukQ2KMG71NSD34nJkNlxNBBlcsTQeYO2p8tBaqtqsvgSuMzWx4nAgG0TvA3GsPamXfOdOLDmcP+86eTlOjXGFsZsPmRDCI2x7u4unnih9gbiCNDWK/uc3uXWxmw+ZEMIjLl3Sw14ypvL7gAeZqSZqQ+tGQmQ2PE0ENqzds5acPPcPbXrKg8AHmakk6lblEYGbDM36/3caBnQPMjbO+A31VWkqs3riNTdu6iw7FzCYgJ4IBRASXL+3gyMpcDtqn+AHmavFE9mY2Ek4EA7in81n+8MyGcVtJXM3DUZvZSDgRDOCyJb0DzM0vOpRBVU9QY2Y2VE4E/di8rYfr7lnOSS+ez6xxNMDcQObNnMr0pga3HDKzYcktEUi6UNJKSfcPsP89ku5NX7dLOiKvWIbq+vtWsGFrN6dPgMdCAJJoK5dcIjCzYcmzRHARcGKN/Y8Br42Iw4EvAotzjGVILl/awaK9ShwzzgaYq6XivgRmNky5JYKIuA1YU2P/7RGxNl29AxgXA/k8vmojv3lsDe8chwPM1VJpKXngOTMblvFSR/Bh4IaBdko6W9JSSUu7urpyDeSKZR3jdoC5WirlEuu3dPPspu1Fh2JmE0zhiUDS60gSwTkDHRMRiyOiPSLaW1tbc4ulu2cHVy7r5LgX7j0uB5irpdKS9CXw4HNmNlSFJgJJhwMXAKdGxOoiYwH45cOreOa5rROi70BfbW5CambDVFgikLQQuBp4X0T8oag4ql2WDjB3/MF7Fx3KkO3sS+ASgZkN0ZS8TizpEuA4YJ6kTuBzQBNARJwP/D2wF/DttFK2OyLa84pnML0DzJ35ykXjeoC5gcwpNTFr+hRPUGNmQ5ZbIoiIMwbZfxZwVl7XH6pr7nqK7h0xIR8L9aqU3XLIzIZu4v30zUFEcNmSDl6ycC4HjvMB5mqptLgvgZkNXc1EIKlR0lfHKpii3N2xjodXTowB5mrpLRFERNGhmNkEUjMRREQP8FJNpJ5Vw3D50g6amxp5y+Hjf4C5WiotJbZs30HXhq1Fh2JmE0iWOoK7gB9JugLY2LsxIq7OLaoxtGlbN9fds2LCDDBXy86+BGs2s/esidUPwsyKkyURtACrgeOrtgVJ088J7/r7nk4GmBvns5Bl0duXoHPtJl76vHLB0ZjZRDFoIoiIM8cikKJcvrSD/efN4OhFE/+Ls3emMncqM7OhGLTVkKQ2SdekQ0o/I+kqSRNrIJ4BPLZqI799bA3vbG+bUAPMDaQ0dQrzZk71lJVmNiRZmo9+F7gW2A9YAFyXbpvwrljaQWODeMcEG2CulrZyyb2LzWxIsiSC1oj4bkR0p6+LgPxGfhsjOweYO6iVvWdPnorVSkvJvYvNbEiyJIJVkt6b9ilolPReksrjCe22h7tYuX4rp02CSuJqlXIzy9dtpmeH+xKYWTZZEsGHgNOAp4EVwDvSbRPaZUs6mDdzYg4wV0tbuUT3jmDFsy4VmFk2NVsNSWoE/ikiThmjeMbEqg1bueWhlXzoVfvT1Di5Rtmo7kvQ25zUzKyWLD2LWyVNHaN4xsQ1d/YOMDd5Kol7Var6EpiZZZGlQ9njwK8kXcvuPYu/lldQeYoILl/awVEL5/KCvSfuAHMD2W9uMxIefM7MMsvyXGQ58OP02FlVrwnprkkywNxApk5pYP7s6XS6U5mZZZSljuDAiHjvGMWTu8uXpAPMHbFf0aHkpq3FfQnMLLu6qiNIBphbzh8dPp+Z03Kbk6dwlbL7EphZdrnVEUi6EHgLsDIiDutnv4CvAycBm4APRsSd2UMfuv+8dwUbt/VMigHmamkrN/PM+i1s7e5h2pTGosMxs3EuzzqCi4ATa+x/M3Bg+job+E6Gc47IFUs7OWDeDNon+ciclZYSEfCUK4zNLIMso49+oe82SVned5ukRTUOORX4j0im07pD0lxJ8yNixWDnHo5Huzbw28fXcM6JB0+KAeZqqaSjkHau3cwBrTMLjsbMxrsBSwSS/qtq+ft9dv92FK69AOioWu9Mt/UXy9mSlkpa2tXVNayLPbxyA+VSE28/qt9LTCqVlqQvgSuMzSyLWr/sZ1Qt933GPxo/qfs7R78D5ETEYmAxQHt7+7AG0Tnh0H05/uC9J11P4v7sM3s6TY1yhbGZZVLrWzEGWO5vfTg6gepa2zaS+ojc1EMSAGhsEAvmNrtEYGaZ1CoRzJX0NpJkMVfSH6fbBcwZhWtfC3xM0qXAscCzedUP1KNKS8mdyswsk1qJ4FbglKrlk6v23TbYiSVdAhwHzJPUCXwOaAKIiPOB60majj5C0nx0Uk+JOdbays3cuPy5osMwswlgwEQw0rmKI+KMQfYH8NGRXMMG1lYusWbjNjZu7WbGJO48Z2YjVx8PzetQb8shz19sZoNxIpikevsSdLiewMwG4UQwSbkvgZllNeDD46pWQv2KiKtHPxwbLXvNmEpzU6P7EpjZoGrVIva2EtobeAXws3T9dcAvACeCcUwSbWX3JTCzwQ3aakjSj4FDetv4S5oPfGtswrORqLSUXFlsZoPKUkewqE9Hr2eAg3KKx0ZRpdxM55pNJC11zcz6l6WB+S8k3QhcQjK0xLuAn+calY2KSkuJ9Vu7eXbzduaWJsXcQmaWgyzDSX8sHWriNemmxRFxTb5h2WhoK6cth9ZsdiIwswFl7XJ6J7A+In4qqSRpVkSszzMwG7lKS9qXYO0mXtw2GsNDmdlkNGgdgaQ/Aa4E/jXdtAD4YY4x2SjZVSJwyyEzG1iWyuKPAq8EngOIiIdJmpTaODenuYnZ06e4CamZ1ZQlEWyNiG29K+k0lW6GMkG4CamZDSZLIrhV0meBZklvBK4Arss3LBstlXLJj4bMrKYsieAcoAu4D/hTknkE/jbPoGz0VFqa6Vy72X0JzGxANVsNSWoA7o2Iw4B/G5uQbDRVWkps7d5B1/qt7D17etHhmNk4VLNEEBE7gHskLRyjeGyUVcoehdTMasvyaGg+8ICkWyRd2/vKcnJJJ0r6vaRHJH2mn/1zJF0n6R5JD0jydJWjbGdfAo9CamYDyNKh7AvDObGkRpLB6d4IdAJLJF0bEQ9WHfZR4MGIOFlSK/B7SRdXt1KykVkwt3emMpcIzKx/WYaYuHWY5z4GeCQiHgWQdClwKlCdCAKYJUnATGAN0D3M61k/mqc2Mm/mNJcIzGxAWXoWv0zSEkkbJG2T1CPpuQznXgB0VK13ptuqfRN4EbCcpFXSJ9N6ib4xnC1pqaSlXV1dGS5t1SotnpfAzAaWpY7gm8AZwMNAM3BWum0w6mdb3zaMJwB3A/sBRwLflDR7jzdFLI6I9ohob21tzXBpq1Ypl5wIzGxAmeYsjohHgMaI6ImI7wLHZXhbJ1CpWm8j+eVf7Uzg6kg8AjwGHJwlJsuu0tLM8nVb6O7Zo7BlZpYpEWySNBW4W9I/S/oLYEaG9y0BDpS0f/r+dwF9Wxs9CbweQNI+wAuBRzNHb5lUyiV6dgQrnt1SdChmNg5lSQTvAxqBjwEbSX7lv32wN0VEd/qeG4GHgMsj4gFJH5H0kfSwLwKvkHQfcAtwTkSsGvptWC1t7ktgZjVkaTX0RLq4mSE2JY2I60mGpKjedn7V8nLgTUM5pw1db18CDz5nZv0ZNBFIeox+RhuNiANyichG3X5zm2kQdHrwOTPrR5YOZe1Vy9OBdwIt+YRjeWhqbGD+nGY6XCIws34MWkcQEaurXk9FxHnA8fmHZqOprdzs4ajNrF9ZHg0dVbXaQFJCmJVbRJaLSkuJXz7sznhmtqcsj4b+pWq5G3gcOC2XaCw3lXKJZ57bypbtPUxvaiw6HDMbR7K0GnrdWARi+WorJy2Hlq/bzAGtMwuOxszGkyyPhv6y1v6I+NrohWN5qbT09iVwIjCz3WVtNXQ0u3oFnwzcxu4Dytk4t2teAlcYm9nusiSCecBREbEeQNLngSsi4qw8A7PRtc+s6UxtbHDvYjPbQ5YhJhYC1RPFbAMW5RKN5aahQSwoN9PpeQnMrI8sJYLvA7+VdE26/lbge7lFZLlpK3teAjPbU5ZWQ1+SdAPwapKhJs6MiLtyj8xGXaWlxP33rSg6DDMbZwZ8NCSpJKkJICLuBH5CMgrp/mMUm42ytnIzazdtZ8NWzwZqZrvUqiP4CWldgKQXAL8GDgA+KunL+Ydmo61S9kT2ZranWomgHBEPp8sfAC6JiI8Dbwb+KPfIbNTt7EvgCmMzq1IrEVQPPX08cDNARGwDPOfhBFQpuy+Bme2pVmXxvZLOBZ4CXgDcBCBp7hjEZTlomTGV0tRGtxwys93UKhH8CbCKpJ7gTRHR++1xCHBuznFZDiRRKZf8aMjMdjNgiSAiNgO7VQpLOioibgduz3JySScCXydpbXRBROxRySzpOOA8oAlYFRGvzRi7DUNbudmVxWa2myw9i6tdkPVASY3At0gqlw8BzpB0SJ9j5gLfBk6JiENJZj+zHFVaSnSu3UzEHrOPmlmdGmoi0BCOPQZ4JCIeTSuYLwVO7XPMu4GrI+JJgIhYOcR4bIjays1s2NrNuk3biw7FzMaJWh3Kvp/+95NVm78whHMvYPcRSjvTbdUOAsqSfiFpmaT3DxDL2ZKWSlra1eVZtkZi13DUfjxkZolaJYKXSnoe8CFJZUktwG2SWtLlwfRXeuj7PGIK8FKSfgknAH8n6aA93hSxOCLaI6K9tbU1w6VtIL2dylxhbGa9ajUfPZ+kd/EBwDJ2/2KPdHstnUClar0NWN7PMasiYiOwUdJtwBHAHwYP3YZj57wELhGYWWrAEkFEfCMiXgRcGBEHRMT+Va/BkgDAEuBASftLmgq8i12T2/T6EfBqSVMklYBjgYeGeS+WwazpTcwtNbnlkJntlGX00T+TdATJ6KMAt0XEvRne1y3pY8CNJM1HL4yIByR9JN1/fkQ8JOknwL0kvZUviIj7h3szlk1budmPhsxspyxzFn8COBu4Ot10saTFEfH/BntvRFwPXN9n2/l91r8KfDVzxDZilXKJ3z+zvugwzGycyDIxzVnAselzfCR9hWQk0kETgY1PlZYSt/xuJTt2BA0NQ2kRbGaTUZZ+BAJ6qtZ7GFp/AhtnKuVmtnXvoGvD1qJDMbNxIEuJ4LvAb9KpKkXSKezfc43KctW2czjqTewze3rB0ZhZ0QYtEUTE14AzgTXp68yIOC/nuCxHO/sSuOWQmZGtRNA7VeWdkt7i+YonvrZ0XoJOtxwyM4Y+1tA/5BKFjanpTY20zprmEoGZAfkOOmfjWMV9CcwsNdRE8Ke5RGFjrtJSconAzIBsHcoaSQaFWwRMkfQq2FmJbBNUpVzix/euoLtnB1Mah/p7wMwmkyyVxdcBW4D78KT1k0alpZmeHcGKZ7fsHJrazOpTlkTQFhGH5x6JjanqJqROBGb1LcszgRskvSn3SGxMtaWJwE1IzSxLieAO4BpJDcB2kpZDERGzc43McjV/7nQa5E5lZpYtEfwL8HLgvvCM55NGU2MD8+c007HGicCs3mV5NPQwcL+TwORTaWmmY60fDZnVuywlghXALyTdAOwcrtLNRye+SrnErX/oKjoMMytYlkTwWPqamr5skqi0lFi5fitbtvcwvamx6HDMrCBZpqr8wnBPLulE4OskU1VeEBFfHuC4o0kqpU+PiCuHez0bmt6J7J9at5nnt84sOBozK0qWnsU/B/aoH4iI4wd5XyPwLeCNQCewRNK1EfFgP8d9hWRuYxtDvU1IO9ZsciIwq2NZHg39r6rl6cDbge4M7zsGeCQiHgWQdCnJpDYP9jnu48BVwNEZzmmjaFenMlcYm9WzLI+GlvXZ9CtJt2Y49wKgo2q9Ezi2+gBJC4C3AcdTIxFIOhs4G2DhwoUZLm1Z7D1rGlOnNNDpJqRmdW3Q5qOSWqpe8ySdAOyb4dz9DVnd9xHTecA5EdHTz7G73hSxOCLaI6K9tbU1w6Uti4YG0Ta32Z3KzOpclkdDy0i+wEXySOgx4MMZ3tcJVKrW24DlfY5pBy6VBDAPOElSd0T8MMP5bRS0tZQ8L4FZncvyaGj/YZ57CXCgpP2Bp4B3Ae8e6NySLgJ+7CQwtirlZu7rXFd0GGZWoAEfDUk6WtK+Vevvl/QjSd+Q1DLYiSOiG/gYSWugh4DLI+IBSR+R9JHRCN5Grq1cYu2m7WzYmqX+38wmo1olgn8F3gAg6TXAl0la+BwJLAbeMdjJI+J64Po+284f4NgPZgnYRldvX4KONZt40XyPI2hWj2pVFjdGxJp0+XRgcURcFRF/B7wg/9BsLFSq+hKYWX2qmQgk9ZYYXg/8rGpflkpmmwB6J6VxXwKz+lXrC/0S4FZJq4DNwC8BJL0AeHYMYrMxUC41MWNqo0sEZnVswEQQEV+SdAswH7ipahjqBpK6ApsEJFFpKdHpvgRmdavmI56IuKOfbX/ILxwrQlvZicCsnmWZmMYmubZyMlOZ5x4yq09OBEalpcTGbT2s3bS96FDMrABOBEalvKsvgZnVHycCq2pC6kRgVo+cCGxXIvDgc2Z1yYnAmDltCuVSk1sOmdUpJwIDklKBexeb1ScnAgOSJqSeqcysPjkRGJAMPte5djM7drgvgVm9cSIwIJmpbFvPDlau31p0KGY2xpwIDKjqS+AKY7O640RgwK4mpG45ZFZ/ck0Ekk6U9HtJj0j6TD/73yPp3vR1u6Qj8ozHBrZgbm/vYrccMqs3uSUCSY3At4A3A4cAZ0g6pM9hjwGvjYjDgS+STIFpBZje1Mjes6Z5mAmzOpRnieAY4JGIeDQitgGXAqdWHxARt0fE2nT1DqAtx3hsEElfAicCs3qTZyJYAHRUrXem2wbyYeCGHOOxQVTKzX40ZFaH8kwE6mdbv43UJb2OJBGcM8D+syUtlbS0q6trFEO0apWWEiue3cz2nh1Fh2JmYyjPRNAJVKrW24DlfQ+SdDhwAXBqRKzu70QRsTgi2iOivbW1NZdgLelUtiNgxbotRYdiZmMoz0SwBDhQ0v6SpgLvAq6tPkDSQuBq4H2eArN4bS1JyyE3ITWrLzXnLB6JiOiW9DHgRqARuDAiHpD0kXT/+cDfA3sB35YE0B0R7XnFZLVVyp6XwKwe5ZYIACLieuD6PtvOr1o+Czgrzxgsu/lzptPYIFcYm9UZ9yy2naY0NjB/znSXCMzqjBOB7aZSLrlTmVmdcSKw3VRamj1BjVmdcSKw3VTKJbrWb2XL9p6iQzGzMeJEYLvZNQqpSwVm9cKJwHZTafG8BGb1xonAdtOW9iXw/MVm9cOJwHbTOnMaU6c0uMLYrI44EdhuGhpEW7nZTUjN6ogTge2hUva8BGb1xInA9lBpaXarIbM64kRge6iUS6zbtJ31W7YXHYqZjQEnAttDb8shDz5nVh+cCGwP7ktgVl+cCGwPO+clcMshs7rgRGB7mFtqYua0Ka4wNqsTTgS2BynpS+ApK83qgxOB9avSUnJlsVmdyDURSDpR0u8lPSLpM/3sl6RvpPvvlXRUnvFYdr2dyiKi6FDMLGe5JQJJjcC3gDcDhwBnSDqkz2FvBg5MX2cD38krHhuatnIzm7b1sGbjtqJDMbOc5Tl5/THAIxHxKICkS4FTgQerjjkV+I9IfnbeIWmupPkRsSLHuCyD3nkJ/vg7tzO10U8QzcaD04+ucNarDxj18+aZCBYAHVXrncCxGY5ZAOyWCCSdTVJiYOHChaMeqO3p2ANaeMdL29i0rbvoUMwsNW/mtFzOm2ciUD/b+j5wznIMEbEYWAzQ3t7uh9ZjYPb0Js595xFFh2FmYyDPMn8nUKlabwOWD+MYMzPLUZ6JYAlwoKT9JU0F3gVc2+eYa4H3p62HXgY86/oBM7OxldujoYjolvQx4EagEbgwIh6Q9JF0//nA9cBJwCPAJuDMvOIxM7P+5VlHQERcT/JlX73t/KrlAD6aZwxmZlab2wWamdU5JwIzszrnRGBmVuecCMzM6pwm2qBikrqAJ4qOYxDzgFVFBzFKJsu9TJb7AN/LeDQR7uN5EdHa344JlwgmAklLI6K96DhGw2S5l8lyH+B7GY8m+n340ZCZWZ1zIjAzq3NOBPlYXHQAo2iy3MtkuQ/wvYxHE/o+XEdgZlbnXCIwM6tzTgRmZnXOiWCUSKpI+rmkhyQ9IOmTRcc0UpIaJd0l6cdFxzIS6RSoV0r6Xfr3eXnRMQ2HpL9I/23dL+kSSdOLjikrSRdKWinp/qptLZJulvRw+t9ykTFmNcC9fDX993WvpGskzS0wxCFzIhg93cCnI+JFwMuAj0o6pOCYRuqTwENFBzEKvg78JCIOBo5gAt6TpAXAJ4D2iDiMZGj3dxUb1ZBcBJzYZ9tngFsi4kDglnR9IriIPe/lZuCwiDgc+APwN2Md1Eg4EYySiFgREXemy+tJvmwWFBvV8ElqA/4IuKDoWEZC0mzgNcC/A0TEtohYV2hQwzcFaJY0BSgxgWbzi4jbgDV9Np8KfC9d/h7w1rGMabj6u5eIuCkieif4voNktsUJw4kgB5IWAS8BflNwKCNxHvDXwI6C4xipA4Au4LvpY64LJM0oOqihioingHOBJ4EVJLP53VRsVCO2T++MhOl/9y44ntHyIeCGooMYCieCUSZpJnAV8KmIeK7oeIZD0luAlRGxrOhYRsEU4CjgOxHxEmAjE+cRxE7p8/NTgf2B/YAZkt5bbFTWl6T/TfKY+OKiYxkKJ4JRJKmJJAlcHBFXFx3PCLwSOEXS48ClwPGSflBsSMPWCXRGRG/p7EqSxDDRvAF4LCK6ImI7cDXwioJjGqlnJM0HSP+7suB4RkTSB4C3AO+JCdZBy4lglEgSyXPohyLia0XHMxIR8TcR0RYRi0gqJH8WERPy12dEPA10SHphuun1wIMFhjRcTwIvk1RK/629nglY6d3HtcAH0uUPAD8qMJYRkXQicA5wSkRsKjqeoXIiGD2vBN5H8uv57vR1UtFBGQAfBy6WdC9wJPBPxYYzdGmJ5krgTuA+kv93J8ywBpIuAX4NvFBSp6QPA18G3ijpYeCN6fq4N8C9fBOYBdyc/r9/fs2TjDMeYsLMrM65RGBmVuecCMzM6pwTgZlZnXMiMDOrc04EZmZ1zonArA9JPVVNgO+WNGo9kSUtqh610mw8mFJ0AGbj0OaIOLLoIMzGiksEZhlJelzSVyT9Nn29IN3+PEm3pGPR3yJpYbp9n3Rs+nvSV++QEI2S/i2dW+AmSc2F3ZQZTgRm/Wnu82jo9Kp9z0XEMSQ9Sc9Lt30T+I90LPqLgW+k278B3BoRR5CMb/RAuv1A4FsRcSiwDnh7rndjNgj3LDbrQ9KGiJjZz/bHgeMj4tF0gMGnI2IvSauA+RGxPd2+IiLmSeoC2iJia9U5FgE3p5OxIOkcoCki/nEMbs2sXy4RmA1NDLA80DH92Vq13IPr6qxgTgRmQ3N61X9/nS7fzq5pI98D/Fe6fAvwZ7Bz/ufZYxWk2VD4l4jZnpol3V21/pOI6G1COk3Sb0h+RJ2RbvsEcKGkvyKZDe3MdPsngcXp6JQ9JElhRd7Bmw2V6wjMMkrrCNojYlXRsZiNJj8aMjOrcy4RmJnVOZcIzMzqnBOBmVmdcyIwM6tzTgRmZnXOicDMrM79D4CpBVut1GUqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "J = perceptron_train(x, y, z, eta, t)[1]     # pulling out the sum-of-squared errors from the tuple\n",
    "epoch = np.linspace(1,len(J),len(J))\n",
    "\n",
    "%matplotlib inline  \n",
    "plt.plot(epoch, J)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Sum-of-Squared Error')\n",
    "plt.title('Perceptron Convergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-extent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
